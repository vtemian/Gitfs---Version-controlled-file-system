\chapter{Theoretical background}

\label{ch:theoretical}
In the next part of the paper we will talk a bit more about the technologies and the concepts used in the making process, in order to offer a better understanding of the project. We'll start by presenting the building parts of Gitfs, from file system to git internals, and end up also discussing about similar projects.

\section{File systems}
    A file system is an abstraction used by the operating systems in order to keep track of files on a storage device. In a more concrete form, you can see the file systems as a collection of data structures and algorithms which helps the operating system to manage data from a storage device. The operating system provides an API, which other programs can use to store, retrieve or delete data, from a storage device.

    The operation of associating a storage device with a file system is called `mounting`. In UNIX based operating systems, one can use the 'mount' command, which will attach a file system to a file tree. Using the 'mount' command, one can specify a file system, its type, specific options used by the file system and a mounting point.

    \subsection{High level architecture}
        The actual structure of a file system can vary based on the implementation, but the majority of UNIX file systems, share a common one. It is composed by two main parts: user space and kernel space.

        User space contains the applications (for this example, the user of the file system) and the GNU C Library (glibc), which provides the user interface for the file system calls (open, read, write, close). The system call interface acts as a switch, funneling system calls from user space to the appropriate endpoints in kernel space.

        \begin{figure}[h]
           \begin{center}
               \includegraphics[width=\textwidth]{theoretical/filesystem-arhitecture.png}
            \end{center}
            \caption{High level architecture of a file system \cite{FSA}}
            \label{fig:filesystem}
        \end{figure}

        As you can see in figure ~\ref{fig:filesystem}, an important part of a file system architecture stays in the kernel space. There, the Virtual File System (VFS) behaves as an interface for other file systems. It behaves like a gateway between system call interface and individual file systems, with some caches. It does that by exposing a series of interfaces to individual file systems, optimizing using two caches: dentries (directory cache) and inode. Each one of those caches stores the recently-used objects (implementing a LRU cache mechanism).

        A kernel space implemented file system won't interact with a device directly. All write and read operations are passed through a cache, called a buffer cache. This is a very smart mechanism that offers a big performance boost, but with a cost. If you use the system call interface in order to write something in a file, that write is not atomic, meaning that if you close the file, the bits that you are writing are not on the storage device, but in this buffer cache. The kernel will flush periodically this cache on the device, but also can control the flushing using fsync system call, after you close the file. The buffer cache will use a LRU (Least Recently Used) cache which means that is a finite small cache that will store the most recent data, being optimized for use cases when you use the same data.

        Linux uses multiple types of data structure to manage the entire file system:
        \begin{itemize}
            \item superblock: the main component of a file system that maintains and describes state in the file system. It contains all the information needed by the file system during certain operations, such as: file system name (e.g.: ext2, ntfs, fat32 etc.), the size of the file system, pointer to a block device and different metadata. It is usually stored on the storage device but it can also be created at runtime. Besides that information, a superblock also contains a set of operations. This structure is used in order to manage the inodes within the file system.
            \item inode: core representation in a file system. Every file or directory is represented as an inode.
            It contains all the meta data needed by the file systems (including which operations are possible on it) to manage a file. Individual file systems will provide different functions and mechanisms in order to generate a unique inode identifier from a filename and later into an inode reference.
            \item dentry: data structure used to map the names of files to inodes. It uses a directory cache to keep the most-recently used data closely. The operating system also uses dentries to travers the file system.
        \end{itemize}

    \subsection{User space}
        Besides programming an individual file system in the kernel space you can use FUSE. FUSE stands for file system in user space and it is an interface for programs which helps you export a file system to the Linux kernel. It has two main components: libfuse and fuse kernel module. Libfuse is a C library which allows you to communicate with the FUSE kernel module.

        After the mounting process of a FUSE-base file system is finished, the entire collection of system calls which target that mountpoint will be redirected to the FUSE kernel module, acting like a proxy.

        \begin{figure}[h]
           \begin{center}
               \includegraphics[width=\textwidth]{theoretical/fuse.png}
            \end{center}
            \caption{System call flow in FUSE \cite{FuseArch}}
            \label{fig:fuse}
        \end{figure}

        Let's take read operation as an example, detalied in ~\ref{fig:fuse}. Let's imagine that the directory /fuse is the FUSE mountpoint. When an application will call the read() syscall with /fuse/file as the path argument, the Virtual File System will trigger the corresponding handler in fusefs. A cache is used in order to optimize the operations flow. If the corresponding data is found in the cache it will be returned. Otherwise, the read() syscall will be forwarded to the libfuse library, which will call the callback defined in userfs for the read() syscall. The callback can make any action and return any data in the supplied buffer. With this mechanism one can implement any desired logic. For example, you can access Facebook's API in order to retrieve some photos, or to get data from an external storage device, or to do some processing from a local file system. After you've finished your work, the result is propagated to libfuse, through kernel and to the actual application which triggered the read() syscall.

        In an UNIX based operating system, only privileged users can mount a file system. In order to allow the same behaviour for non priviledged users, the fusemount program was created. It also offers the possibility to customize several parameters when one mounts a file system.

        FUSE exposes two different APIs used to develop the file system:
        \begin{itemize}
            \item low-level: using this API the file system will need to manage the inodes and path translations (also caching for the translations), copying the VFS interface very closely. Also, the file system will need to manually fill and use the data structures used to communicate with the kernel. This API is mostly used to develop file system from scratch (take a look at ZFS-FUSE), in contrast to those that are just adding some functionality to existing file systems (extending them).
            \item high-level: opposing the low-level API, using this level will allow the file system developer to work with pathnames, not having to deal with inodes and dentries, copying more the system calls interfaces than Virtual File System interface. Also, there is no need for any caching mechanism for inode-to-pathname translations and the data structures used for kernel communication are already filled. This API is used by the majority of FUSE file systems, taking care of most of the work in order to let the file system developer build the desired logic.
        \end{itemize}

        The majority of FUSE file systems are implemented as standalone applications that link with libfuse and use the high-level API to implement a specific behaviour. Because of the FUSE (kernel module) acts like a proxy to a libfuse based application, the performance is drastically lower than a usual kernel file system. Even though the performance may be lower, the speed of development is drastically improved. Libfuse has bindings in all major high level languages. Also, because FUSE will take care of inodes, dentries and caching the inode-to-pathname translations, the margin for errors is drastically reduced.

\section{Git}
    Git is one of the most popular distributed version control systems and it is also free and open source. It was originally created by Linus Torvalds with the purpose of improving Linux development which had become very cumbersome to maintain, due to the large number of patches that needed to be reviewed and integrated in the kernel.

    The central component of git is the repository. A repository is a directory which contains all the necessary files that git uses to manage the content and the history. This repository can be bare, containing only the necessary parts without the content itself, or non-bare and including the content.

    In order to get a repository on the local system you need to clone it. You can clone a repository from a local machine or a remote one. Git can use different protocols in order to clone a repository (ssh, http/https or git protocol). Every change that was managed by git can be pushed to a remote repository. Each repository, on which you want to push or from which you want to pull changes, has an address and a name. The data structure that encapsulates these attributes is called a remote. You can have multiple remotes on a local repository, with different addresses but with unique local names. The default remote (initially used to clone the repository) is called origin.

    Using remotes you can distribute the code and work offline. Changes can be synchronized between multiple remotes and any raising conflicts would be solved automatically (if possible).

    \subsection{Overview}
        After a remote repository is cloned on the local machine, one can start changing its content. Git will track only the files that are already in the repository. Newly created files need to be manually added as tracked. Empty directories are not track-able.

        Any new changes are viewed and tracked by git, but not automatically transformed in a version. In order to group the changes, git uses the concept of stage. Each new change is tracked by git in the so called working stage. In this zone, the changes are very volatile. If you want to pull content from a different remote, git will stop you in order to save the local changes. In order to group them, you need to add the changed files to the staging area, by using `git add` command.

        Once you group the changes together, by moving the files to the stage area, you can create a version. That version is named commit. A commit is a git object, which contains a group of files that were changed, the name of who made the changes, who created the commit and the time of when these happened. Besides this metadata, git will attach an unique hash to it in order to be easily identifiable and a reference to the previous commit, called parent. In this way, git can offer an ordered log (not by time, but the order of changes, because you can rewrite the history and relocate a given commit in its tree of changes) giving you a broader understanding of the changes that were made.

        Given the fact that each commit has a parent, you can easily form a chain of commits, called a branch. You can create as many branches as you want, starting with a commit. The default branch is considered to be the master. A branch can be created just locally and pushed to any remote repository afterwards. The last commit from a given branch is called HEAD.
        You can change the HEAD at any time, changing the state of the local or remote repository.

        \begin{figure}[h]
           \begin{center}
               \includegraphics[width=\textwidth]{theoretical/git_flow.png}
            \end{center}
            \caption{Git's chain of commits \cite{Commits}}
            \label{fig:flow}
        \end{figure}

        From a commit two branches can diverge, as showed in figure ~\ref{fig:flow}. If you want to bring commits from another branch to a given current branch, you can do that by using one of two methods: rebasing or merging.
        Using the rebase method you will apply the current commits (set of changes) to the last commit of the branch you want to rebase into, in the order of which the commits were created, one by one. Any conflicts will be solved at commit level, if this will be the case.
        The merging method will try to apply the changes without keeping track of the order of the commits. All conflicts will be solved at the end of the operation and will end with a commit.
        The safest and cleanest method is the rebase method, but a big inconvenience is that it will rewrite the history, creating new commits (with the same data and metadata: date, author, commiter etc.) for each commit applied.

        You can use any of these methods in order to apply remote changes to current state of the content. Git can manage the branches from all the remotes and using some porcelain commands, you can decide which changes to be applied to the local content.

        Another useful structure offered by git is the tag. A tag is a frozen reference which will point to given state in the repository, just like a commit. Unlike a commit, a tag can't be changed and has no parent. Tags are used in order to froze the state of a given repository.

        You can imagine the entire repository like a tree, with multiple branches. The local repository will point to a certain reference. This reference can be a tag, the head of a branch or just the hash of a commit. In this way, you can easily change the state of your repository, by checking-out on different references. The checkout operation can have multiple modes (soft, hard) and in case of conflicts, you can solve them manually or by accepting the local changes (referred as ours) or the remote ones (referred as theirs).

    \subsection{Internals}
        Until now we have talked about git more from a functional point of view, like a tool. But git can be viewed like a database as well or, more precisely, like a key-value database. In this way, one can refer to git as a content addressable file system. You can add any type of content into it and receive a key which you can use to retrieve the content back. In order to investigate this further we need to take a look at dot git directory.

        Dot git directory contains all the data and the meta data of our repository. The part of meta data is stored in the very descriptive files. Some of the most important are:

        \begin{itemize}
            \item HEAD: the current reference to which your repository points. Usually it will be refs/heads/master
            \item refs: all known references, grouped by remotes. Here you will also find the mapping between branches and commits.
            \item index: the staging area with it's meta data, like file names, timestamps and hashes of the files managed by git.
        \end{itemize}

        Beside those meta data files, dot git directory hides the entire repository's state and history in a directory called objects. Git will store every file ever changed in two type of data structures: tree and blob. Each such object is represented by an unique hash. SHA-1 hash function is used, over the content and the meta data of an object, in order to generate a hash. That hash is used to map an object to it's content. The content is archived (using the zip format) and stored on the disk, in objects directory using the hash as name for the file.

        It's very similar to how an UNIX file system manages the files. The tree objects will correspond to an UNIX directory and a blob to, more or less, an inode or file contents. Every tree object can contain one or more trees, each of them contains a SHA-1 pointer to a blob or a subtree, having associated file name, mode and type. Git will create those objects by retrieving the state of your repository (staging area and index) and writing tree objects from them. It will create new objects only for changed files and reuse old nodes if their corresponding files were not changed. In this way git can optimize disk memory usage.

        \begin{figure}[h]
           \begin{center}
               \includegraphics[width=\textwidth]{theoretical/data-model-2.png}
            \end{center}
            \caption{\label{fig:gitobjects} Git data model \cite{GOBJ}}
        \end{figure}

        As git will not store empty directories, the leafs will be all the time blobs, not trees, as showed in figure ~\ref{fig:gitobjects}.

        As one can observe, by using this mechanism a tree is formed for the current state of the repository (and it will be used to compose other version of the repository). This tree data structure is based on Merkle tree, a type of tree in which all the nodes are uniquely identified by a hash, except the leafs. This data structure is very efficient for lookup operations, which can be done in O(logn) time. In this kind of tree, if a node's hash is changed, all the nodes up to the root need to update their hashes. In this way, git will create a new tree for each commit you create, but in that tree, the nodes that were not changed, will be reused.

        Another important object is the commit object. This object will store a reference (SHA-1 hash) to the root of a the current tree (which represents a snapshot of the current state of the repository), a reference to the previous commit (called parent), the author and commiter (can be separate persons) and a blank line followed by the commit message. In this manner, a graph will be formed, as showed in figure ~\ref{fig:gitobjectsrelations}.

         \begin{figure}[h]
           \begin{center}
               \includegraphics[width=\textwidth]{theoretical/data-model-3.png}
            \end{center}
            \caption{\label{fig:gitobjectsrelations} Git's objects and their relations \cite{GDM}}
        \end{figure}

\section{Python}
    Python is a mature and popular programming language. It's main strengths are: readbility, dynamically typed, high-level, interpreted and multi-purpose. It was designed with a strong accent on code readability and clean syntax which makes it a very easy language to learn and maintain. The language has a lot of constructs intended to create clear and readable small and large scale programs. Using those constructs programmers can express complex ideas much faster, cleaner and in fewer lines of code, compared to other high-level languages as Java, C\# or C++. It's ancestor, abc language, was created with the purpose to be a teaching language and Python inherited a large part of it.

    One of its first usages (remaining today as an important and large one) was as a scripting language, used for servers administration. Being a high-level programing language it also supports different programming paradigms: object-oriented, functional, imperative or procedural. As a scripting language, it has a dynamic type system and automatic memory management with a large standard library.

    Starting first as a scripting language used for server administration, a lot of system programming features emerged, making it a strong candidate for file systems development.

    Python is the specification of a language. Its implementation varies, depending on which subset one wants to implement and in what language. The reference implementation in C language is called CPython. It is the most popular one and it is found already installed in almost any Linux distribution. PSF (Python Software Foundation, non-profit) manages all CPython development.

      \subsection{Advantages}
        \begin{enumerate}
          \item It is easy to learn and maintain: being developed originally as a learning tool to teach programming, it's still presenting easy to understand constructs and developers can develop complex software faster and easier. Beside being easy to learn and use, Python is very opinionated when it comes to readability by forcing correct indentation of code (using multiple of two spaces). Any control braces had been removed, a block of instructions been defined by the indentation level. Python also come with a set of rules which should be followed by any Python programmer, in Zen Of Python. In PEP (Python Enhancement Proposal) 8 \cite{PEP8} Guido van Rossum, the creator of Python, describes the code style which one should follow when writes any Python software.
          \item It is a dynamically typed language. In a dynamically typed language you don't have to declare a type for a variable first and then assign to that variable just values of that type (like one need to proceed in C, Java or C\#). Python, on the other hand, doesn't provide this behavior. In Python, being a dynamic typed language, a variable name isn't bound to an object and a type, but only to an object. Let's take an example. In a statically typed language, the construct `name = "First Name"` is conditioned by declaring a type for variable `name` (`String name;`). In a dynamically typed language you could easily change the object to which a variable name is referencing, even if the type of the objects would be different (`name = "First Name"`, followed by `name = 5` will work in a dynamically typed language, but not in  a static one ). This kind of behaviour can be very benefic for the speed of the development and can speed up testing. Also, can lead to a lot of confusion and unpredictable scenarios.
          \item It is strongly typed, meaning that variable can't implicit be coerced to unrelated types, an explicit conversion being needed. By unrelated types on should understand types which don't share any structure or logic (string versus int). Some languages, including Python, allow conversion between related type (float and int).
           \item In Python, everything is an objects, include classes. This behaviour can lead to some interesting constructs, like metaprograming, but could lead to a lot of confusion and slow code.
           \item It's not specialized to solve a certain problem, being used in any field (from scientific work, to system programming). Being an lot language, it has a big and warm community, with lots of libraries and tool that makes yourr life easier. Also, it support C bindings, which empower developers to use any C library they want.
           \item All Python code needs to be compiled into byte code. That byte code will be executed by the virtual machine. In this way, the precompiled code could run on different platforms without any hesitation.
        \end{enumerate}

    \subsection{CFFI}
        In Python, one can use C code to speed up the application. In order to do that, Python offers multiple ways, but one of the most used is CFFI (C Foreign Function Interface).

        Foreign Function Interface gives developers the oportunity to use code written in other languages (refered as guests languages, C for example), by accessing and invoking functions in the language in which they wrote their application (for example Python, called the "host" language). In this sense, the term "foreign" is used to describe the fact that the code invoke through functons, is written in another programming language.

        One can access to more then just function. A proper FFI implementation and support can expose global variables, invoking functions in host language as callbacks, from the guest language and convert data structure from host and guest languages.

        The FFI is accessing the library's binary code (its .so file), without to compile any code. In this manner, for Python it isn't possible to make use of different library's compile-time features (e.g. C preprocessro macros and constants).
        Depending on the language and how it implements FFI, one can use some compile-time features, by compiling to C code. This is usually used by compiled languages, Python not being one of them.

        The modes in which CFFI is used are: API (Application Programm Interface) and ABI (Application Binary Interface). Each of those modes come with two types of preparation (compilation): in-line and out-of-line. In this way, the total number in which the CFFI is used is four. The API mode accesses libraries with a C compiler, whereas the ABI mode accesses them at the binery level.
        The difference between out-of-line mode and in-line mode is that in the out-of-line mode produces a module which is importable by your code, produced by a possible C compilation step. On the ohter hand, in the in-line mode, no compilation is needed, the developer just needs to import the Python code, everything being already configured.
    \subsection{Fusepy}
        Fusepy is a Python library, which offers the posibility to invoke Libfuse functions and use Libfuse's logic. It does that using ctypes.

        The original version of fusepy was hosted on Google Code, but is now officially hosted on GitHub.

        Fusepy is written in 2x syntax, but it is trying to also pay attention to bytes and other changes 3x would care about. The only incompatible changes between 2x and 3x are the changes in syntax for number literals and exceptions.
    \subsection{Libgit2}
        Libgit2 is a library written in C which manipulates the git objects at as low level as possible. It follows git implementation and offers an API which helps developers in creating applications that use git, without the use of the git binary.

        A great advantage of Libgit2 is that it is written in C, offering bindings in all major high level languages, using CFFI (C Foreign Function Interface). It offers features like, cloning a repository, creating commits, merging commits and fetching changes from different remote repositories.

    \subsection{Pygit2}
        One of the languages in which Libgit2 has bindings is Python. The implementation of Libgit2 in Python is called Pygit2. It uses CFFI and Python C Objects in order to bring the functionalists of Libgit2 into Python world.

        It is a portable library, having releases for Linux, Windows and MacOS, with a small community, but plenty of contributions.

        Because Python is an object oriented programming language, Pygit2 exposes all of Libgit2's data structures as classes. In this way, Pygit2 offers a mechanism which allows you to extend the functionalists from Libgit2. From those data types, the Repository is the one which encapsulates the most of part the behaviour, using other data types like Remote, Branch and Commit.

\section{Similar Projects}
    \subsection{ZFS}
        ZFS is a file system, designed by Sun Microsystems, combining the separate roles of logical volume manager with a file system. It is one of the most advance and complete file system and come with support for high storage capacities, pool storage (zpool - volumes management), snapshots, copy-on-write, data compression, self healing and data integrity.

        One can use ZFS in order to create versioned content, but it need to have some technical knowledge. Using ZFS snapshots, one can see the history of its content, but not very granular (being limited by the frequency of which ZFS creates the snapshots, the most frequent being at 15 minutes).

        Snapshots are read-only copies of a volume or file system. The process of snapshotting is almos instantly, and initial, the snapshot don't consume any additional disk space. In order for one to access the content of a snapshot, one must mount using a different mount point.

        Those snapshots are persistent during reboots, but can't be shared as easily. One can send a snapshot to another ZFS pool, in order to create a replica of the environment. It is used in primary-replica scenario, in which the replica is in read-only mode and can't record any change, in order to send them to primary. In this manner, one can observe that ZFS is lacking the ability to share changes across multiple ZFS instances.

    \subsection{SparkleShare}
        SparkleShare is an open source software which offers you the possibility to share a directory with more collaborators and create versioned content. It is presented as a standalone application in which an user can setup multiple projects (different directories) which will get synchronized with other collaborators' changes.

        It uses git as storage layer, which gives you the possibility to control where your data is stored, in order to protect your privacy. To make setup easier, the team which developed SparkleShare also developed a series of scripts which automate private git host creation. Beside those scripts, you can use any git hosting services, from Github, Bitbucket (which are proprietary solutions) to Gitlab or Gogs (open source solutions).
        In order to help you maintain the privacy it also includes client side encryption.

        It is written in C\# and it uses git command line tool for git operations. Being written in C\# it has some drawback for portability, but it can compensate with higher code maintainability and extensibility options, and also performance boost. Unfortunately, the performance boost given by C\#, will be annulled by the overhead in using the git command line tool. Spawning processes for each git command can lead to complicate error handling logic and resource heavy usage, because you rely on the git's command line tool implementation.

        Beside file synchronization, it also offers the possibility to revert files. It is suitable for small files (from text to media files), but that great when it come to large binary files, full computer backups or large collection of files.
